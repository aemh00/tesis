{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/astro/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/miniconda3/envs/astro/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/miniconda3/envs/astro/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/miniconda3/envs/astro/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/miniconda3/envs/astro/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import Compose, Normalize, Pad, RandomCrop, RandomHorizontalFlip, ToTensor\n",
    "\n",
    "import ignite.distributed as idist\n",
    "from ignite.contrib.engines import common\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Engine, Events, create_supervised_evaluator\n",
    "#from ignite.metrics import Accuracy, Precision\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/astro/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../imbalanced_dataset_sampler/torchsampler/\")\n",
    "sys.path.append(\"../libs\")\n",
    "from vvv_utils import parse_metadata, parse_light_curve_data, plot_light_curve, get_train_test_ids\n",
    "from imbalanced import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_datasets(path):\n",
    "    train_ds = datasets.CIFAR10(root=path, train=True, download=True, transform=train_transform)\n",
    "    test_ds = datasets.CIFAR10(root=path, train=False, download=False, transform=test_transform)\n",
    "\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "def get_model(name):\n",
    "    if name in models.__dict__:\n",
    "        fn = models.__dict__[name]\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown model name {}\".format(name))\n",
    "\n",
    "    return fn(num_classes=10)\n",
    "\n",
    "\n",
    "def get_dataflow(config):\n",
    "\n",
    "    # - Get train/test datasets\n",
    "    if idist.get_rank() > 0:\n",
    "        # Ensure that only rank 0 download the dataset\n",
    "        idist.barrier()\n",
    "\n",
    "    train_dataset, test_dataset = get_train_test_datasets(config.get(\"data_path\", \".\"))\n",
    "\n",
    "    if idist.get_rank() == 0:\n",
    "        # Ensure that only rank 0 download the dataset\n",
    "        idist.barrier()\n",
    "\n",
    "    # Setup data loader also adapted to distributed config: nccl, gloo, xla-tpu\n",
    "    train_loader = idist.auto_dataloader(\n",
    "        train_dataset,\n",
    "        batch_size=config.get(\"batch_size\", 512),\n",
    "        num_workers=config.get(\"num_workers\", 8),\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    config[\"num_iters_per_epoch\"] = len(train_loader)\n",
    "\n",
    "    test_loader = idist.auto_dataloader(\n",
    "        test_dataset,\n",
    "        batch_size=2 * config.get(\"batch_size\", 512),\n",
    "        num_workers=config.get(\"num_workers\", 8),\n",
    "        shuffle=False,\n",
    "        pin_memory=\"cuda\" in idist.device().type,\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def initialize(config):\n",
    "    model = get_model(config[\"model\"])\n",
    "    # Adapt model for distributed settings if configured\n",
    "    model = idist.auto_model(model)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.get(\"learning_rate\", 0.1),\n",
    "        momentum=config.get(\"momentum\", 0.9),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        nesterov=True,\n",
    "    )\n",
    "    optimizer = idist.auto_optim(optimizer)\n",
    "    criterion = nn.CrossEntropyLoss().to(idist.device())\n",
    "\n",
    "    le = config[\"num_iters_per_epoch\"]\n",
    "    lr_scheduler = StepLR(optimizer, step_size=le, gamma=0.9)\n",
    "\n",
    "    return model, optimizer, criterion, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, optimizer, criterion, lr_scheduler, config):\n",
    "\n",
    "    # Define any training logic for iteration update\n",
    "    def train_step(engine, batch):\n",
    "        x, y = batch[0].to(idist.device()), batch[1].to(idist.device())\n",
    "\n",
    "        model.train()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    # Define trainer engine\n",
    "    trainer = Engine(train_step)\n",
    "\n",
    "    if idist.get_rank() == 0:\n",
    "        # Add any custom handlers\n",
    "        @trainer.on(Events.ITERATION_COMPLETED(every=200))\n",
    "        def save_checkpoint():\n",
    "            fp = Path(config.get(\"output_path\", \"output\")) / \"checkpoint.pt\"\n",
    "            torch.save(model.state_dict(), fp)\n",
    "\n",
    "        # Add progress bar showing batch loss value\n",
    "        ProgressBar().attach(trainer, output_transform=lambda x: {\"batch loss\": x})\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(local_rank, config):\n",
    "\n",
    "    # Setup dataflow and\n",
    "    train_loader, val_loader = get_dataflow(config)\n",
    "    model, optimizer, criterion, lr_scheduler = initialize(config)\n",
    "\n",
    "    # Setup model trainer and evaluator\n",
    "    trainer = create_trainer(model, optimizer, criterion, lr_scheduler, config)\n",
    "    evaluator = create_supervised_evaluator(model, metrics={\"accuracy\": Accuracy()}, device=idist.device())\n",
    "\n",
    "    # Run model evaluation every 3 epochs and show results\n",
    "    @trainer.on(Events.EPOCH_COMPLETED(every=3))\n",
    "    def evaluate_model():\n",
    "        state = evaluator.run(val_loader)\n",
    "        if idist.get_rank() == 0:\n",
    "            print(state.metrics)\n",
    "\n",
    "    # Setup tensorboard experiment tracking\n",
    "    if idist.get_rank() == 0:\n",
    "        tb_logger = common.setup_tb_logging(\n",
    "            config.get(\"output_path\", \"output\"), trainer, optimizer, evaluators={\"validation\": evaluator},\n",
    "        )\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=config.get(\"max_epochs\", 3))\n",
    "\n",
    "    if idist.get_rank() == 0:\n",
    "        tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astro",
   "language": "python",
   "name": "astro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
